knitr::opts_chunk$set(echo = TRUE)
# Clear the workspace
rm(list = ls())
# Identify the workbench (in case of Jeremy's laptop or desktop)
#wd.path1 <- "C:/Users/jlobr/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
wd.path2 <- "G:/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
library(tm)
library(RTextTools)
library(dplyr)
library(ggplot2)
library(magrittr)
library(stringr)
library(tidyr)
library(SnowballC)
# Identify the working directory - we worked collaboratively but with local files, so we flipped this switch when iterating
# Kavya WD
#setwd <- "C:/Users/Kavya/Desktop/Education/msds/DATA 607 Data Acquisition and Management/Projects/Project 04"
# Jeremy's WD
setwd(file.path(wd.path2, "Projects", "Project 4"))
knitr::opts_chunk$set(echo = TRUE)
# Clear the workspace
rm(list = ls())
# Identify the workbench (in case of Jeremy's laptop or desktop)
#wd.path1 <- "C:/Users/jlobr/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
wd.path2 <- "G:/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
library(tm)
library(RTextTools)
library(dplyr)
library(ggplot2)
library(magrittr)
library(stringr)
library(tidyr)
library(SnowballC)
# Identify the working directory - we worked collaboratively but with local files, so we flipped this switch when iterating
# Kavya WD
setwd <- "C:/Users/Kavya/Desktop/Education/msds/DATA 607 Data Acquisition and Management/Projects/Project 04"
# Jeremy's WD
#setwd(file.path(wd.path2, "Projects", "Project 4"))
# The name of the folders in our working directories -- in this case, "spam" and "ham" -- become the `path` argument below.
ham.list <- list.files(path = "spam/", full.names = T, recursive = F)
spam.list <- list.files(path = "ham/", full.names = T, recursive = F)
ham.sapply <- sapply(ham.list, readLines, warn = F)
spam.sapply <- sapply(spam.list, readLines, warn = F)
# Build a dataframe composed of the text read in from the ham and spam emails.
combined1 <- c(ham.sapply, spam.sapply)
combined2 <- data.frame(t(sapply(combined1,c)))
combined.df <- gather(combined2, "file", "text", 1:2796)
# Coerce the text variable to character type
combined.df$text <- as.character(combined.df$text)
# Prime the type variable
combined.df$type <- NA
# Set ranges to map ham and spam values to row ranges within the dataframe
ham.l <- length(ham.sapply)
combined.l <- nrow(combined.df)
# Map the appropriate values to type in the dataframe
combined.df$type[1:ham.l] <- "spam"
combined.df$type[(ham.l + 1):(combined.l)] <- "ham"
combined.df$type <- factor(combined.df$type)
combined.df$text <-  str_replace_all(combined.df$text, "^c\\(", "") %>% # Remove
str_replace_all("[[:punct:]]", " ") %>% # Remove punctuation
str_replace_all("\\s{2,}", " ") # Truncate white space (to single space)
combined.df$text <- enc2utf8(combined.df$text)
combined.corpus <- Corpus(VectorSource(combined.df$text))
combined.corpus <- sample(combined.corpus)
print(combined.corpus)
corpus.dtm <- DocumentTermMatrix(combined.corpus)
corpus.dtm <- removeSparseTerms(corpus.dtm, (1 - 10 / length(combined.corpus)))
print(corpus.dtm)
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The total number of documents -- 2,796
N <- length(combined.corpus)
# The percentage of the data to partition
P.70 <- 0.7 # For a 30% test holdout
P.80 <- 0.8 # For a 20% test holdout
P.90 <- 0.9 # For a 10% test holdout
# Number of documents in the training set
trainSize.70 <- round(P.70*N)
trainSize.80 <- round(P.80*N)
trainSize.90 <- round(P.90*N)
# Number of documents in the test (holdout) set
testSize.70 <- N - round(P.70*N+1)
testSize.80 <- N - round(P.80*N+1)
testSize.90 <- N - round(P.90*N+1)
paste0("For a 30% test holdout, training set = ", trainSize.70, " and test set = ", testSize.70)
paste0("For a 20% test holdout, training set = ", trainSize.80, " and test set = ", testSize.80)
paste0("For a 10% test holdout, training set = ", trainSize.90, " and test set = ", testSize.90)
# Create a container for the 30% test holdout
container.70 <- create_container(corpus.dtm,
labels = labels.corpus,
trainSize = 1:trainSize.70,
testSize = (trainSize.70+1):N,
virgin = F)
slotNames(container.70)
# Create containers for the 20% and 10% test holdouts
container.80 <- create_container(corpus.dtm,
labels = labels.corpus,
trainSize = 1:trainSize.80,
testSize = (trainSize.80+1):N,
virgin = F)
container.90 <- create_container(corpus.dtm,
labels = labels.corpus,
trainSize = 1:trainSize.90,
testSize = (trainSize.90+1):N,
virgin = F)
# Classify using the Support Vector Machines model
svm_model.70 <- train_model(container.70, "SVM")
svm_out.70 <- classify_model(container.70, svm_model)
# Classify using the Support Vector Machines model
svm_model.70 <- train_model(container.70, "SVM")
svm_out.70 <- classify_model(container.70, svm_model.70)
svm_model.80 <- train_model(container.80, "SVM")
svm_out.80 <- classify_model(container.80, svm_model.80)
svm_model.90 <- train_model(container.90, "SVM")
svm_out.90 <- classify_model(container.90, svm_model.90)
# Classify using the Maximum Entropy model
maxent_model.70 <- train_model(container.70, "MAXENT")
maxent_out.70 <- classify_model(container.70, maxent_model.70)
maxent_model.80 <- train_model(container.80, "MAXENT")
maxent_out.80 <- classify_model(container.80, maxent_model.80)
maxent_model.90 <- train_model(container.90, "MAXENT")
maxent_out.90 <- classify_model(container.90, maxent_model.90)
labels.out <- data.frame(
correct.label = labels.corpus[round(P.70*N+1):N], #1
svm.70 = as.character(svm_out.70[,1]),            #2
svm.80 = as.character(svm_out.80[,1]),            #3
svm.90 = as.character(svm_out.90[,1]),            #4
maxent.70 = as.character(maxent_out.70[,1]),      #5
maxent.80 = as.character(maxent_out.80[,1]),      #6
maxent.90 = as.character(maxent_out.90[,1]),      #7
stringsAsFactors = F)
labels.out.70 <- data.frame(
correct.label = labels.corpus[round(P.70*N+1):N],
svm.70 = as.character(svm_out.70[,1]),
maxent.70 = as.character(maxent_out.70[,1]),
stringsAsFactors = F)
head(labels.out.70, 2)
labels.out.80 <- data.frame(
correct.label = labels.corpus[round(P.80*N+1):N],
svm.80 = as.character(svm_out.80[,1]),
maxent.80 = as.character(maxent_out.80[,1]),
stringsAsFactors = F)
head(labels.out.80, 2)
labels.out.90 <- data.frame(
correct.label = labels.corpus[round(P.90*N+1):N],
svm.90 = as.character(svm_out.90[,1]),
maxent.90 = as.character(maxent_out.90[,1]),
stringsAsFactors = F)
head(labels.out.90, 2)
table(labels.out.70[,1] == labels.out.70[,2])
prop.table(table(labels.out.70[,1] == labels.out.70[,2]))
table(labels.out.80[,1] == labels.out.80[,2])
prop.table(table(labels.out.80[,1] == labels.out.80[,2]))
table(labels.out.90[,1] == labels.out.90[,2])
prop.table(table(labels.out.90[,1] == labels.out.90[,2]))
table(labels.out.70[,1] == labels.out.70[,3])
prop.table(table(labels.out.70[,1] == labels.out.70[,3]))
table(labels.out.80[,1] == labels.out.80[,3])
prop.table(table(labels.out.80[,1] == labels.out.80[,3]))
table(labels.out.90[,1] == labels.out.90[,3])
prop.table(table(labels.out.90[,1] == labels.out.90[,3]))
table(labels.out.70[,1] == labels.out.70[,2])
prop.table(table(labels.out.70[,1] == labels.out.70[,2]))
table(labels.out.80[,1] == labels.out.80[,2])
prop.table(table(labels.out.80[,1] == labels.out.80[,2]))
table(labels.out.90[,1] == labels.out.90[,2])
prop.table(table(labels.out.90[,1] == labels.out.90[,2]))
#table(labels.out.70[,1] == labels.out.70[,2])
prop.table(table(labels.out.70[,1] == labels.out.70[,2]))
#table(labels.out.80[,1] == labels.out.80[,2])
prop.table(table(labels.out.80[,1] == labels.out.80[,2]))
#table(labels.out.90[,1] == labels.out.90[,2])
prop.table(table(labels.out.90[,1] == labels.out.90[,2]))
table(labels.out.70[,1] == labels.out.70[,2])
prop.table(table(labels.out.70[,1] == labels.out.70[,2]))
table(labels.out.80[,1] == labels.out.80[,2])
prop.table(table(labels.out.80[,1] == labels.out.80[,2]))
table(labels.out.90[,1] == labels.out.90[,2])
prop.table(table(labels.out.90[,1] == labels.out.90[,2]))
