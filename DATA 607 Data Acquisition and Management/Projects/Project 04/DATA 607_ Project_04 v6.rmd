---
title: "DATA 607, Project 4"
date: "March 25, 2018"
output:
  html_document:
    theme: yeti
    highlight: haddock
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>

# Background

### A. Purpose

The purpose of this assignment.....

<br>

### B. Approach

1. Read in the data.

2. Create a corpus.
   
3. Clean the data.

4. Create a document-term matrix.

5. Create a container.

6. Train the data.

7. Evaluate the results.

<br>

### B. Libraries and Setup

```{r, echo = F}

rm(list = ls())

# Identify the workbench
#wd.path1 <- "C:/Users/jlobr/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
#wd.path2 <- "G:/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
```


```{r message=FALSE, warning=FALSE}
library(tm)
library(RTextTools)
library(dplyr)
library(ggplot2)
library(magrittr)
library(stringr)
library(tidyr)
library(SnowballC)
```

<br>

****

# 1. Read in the data.

### A. Set the working directory to be the location of the extracted ham and spam folders.

```{r}

setwd <- "C:/Users/Kavya/Desktop/Education/msds/DATA 607 Data Acquisition and Management/Projects/Project 04"

#setwd(file.path(wd.path2, "Projects", "Project 4"))

```

<br>

### B. Create separate lists of spam and ham filepaths.

The name of the folders in your working directory -- in this case, "spam" and "ham" -- become the `path` argument below.

```{r}

ham.list <- list.files(path = "spam/", full.names = T, recursive = F)

spam.list <- list.files(path = "ham/", full.names = T, recursive = F)

```

<br>

### C. Apply the `readLines` function to every filepath to get the contents of the file (i.e., the email itself).

```{r}

ham.sapply <- sapply(ham.list, readLines, warn = F)

spam.sapply <- sapply(spam.list, readLines, warn = F)

```

<br>

****

# 2. Create a corpus

### A. Combine the two lists of files and create a dataframe.

```{r}

combined1 <- c(ham.sapply, spam.sapply)

combined2 <- data.frame(t(sapply(combined1,c)))

combined.df <- gather(combined2, "file", "text", 1:2796)

combined.df$text <- as.character(combined.df$text)

```

<br>

### B. Add a column for "type" and label the documents as spam or ham.

```{r}

combined.df$type <- NA

combined.df$type[1:1396] <- "spam"

combined.df$type[1397:2796] <- "ham"

combined.df$type <- factor(combined.df$type)

```

<br>

### C. Perform some basic cleaning

```{r}
combined.df$text <-  str_replace_all(combined.df$text, "^c\\(", "") %>% 
                     str_replace_all("[[:punct:]]", " ")

combined.df$text <- enc2utf8(combined.df$text)

head(combined.df, 1)

```

<br>

### D. Create a corpus

```{r}

combined.corpus <- Corpus(VectorSource(combined.df$text))

combined.corpus <- sample(combined.corpus)

print(combined.corpus)

```

****

<br>

# 3. Create a document-term matrix.

```{r}

corpus.dtm <- DocumentTermMatrix(combined.corpus)

corpus.dtm <- removeSparseTerms(corpus.dtm, (1 - 10 / length(combined.corpus)))

print(corpus.dtm)


```

****

<br>

# 4. Create a container.

### A. Set the parameters.

```{r}

# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type

# The total number of documents -- 2,786
N <- length(combined.corpus)

# The percentage of the data to partition
P <- 0.7

# Number of documents in the training set
trainSize <- round(P*N)

trainSize

# Number of documents in the test (holdout) set
testSize <- N - round(P*N+1)

testSize

```

<br>

### B. Create the container.

```{r}

container <- create_container(corpus.dtm, 
                              labels = labels.corpus, 
                              trainSize = 1:round(P*N), 
                              testSize = round(P*N+1):N,
                              virgin = F)

slotNames(container)

```


****

<br>

# 5. Train the data.

```{r}
# Classify using the Support Vector Machines model
svm_model <- train_model(container, "SVM")
svm_out <- classify_model(container, svm_model)

# Classify using the Random Forest model
#tree_model <- train_model(container, "TREE")
#tree_out <- classify_model(container, tree_model)

# Classify using the Maximum Entropy model
maxent_model <- train_model(container, "MAXENT")
maxent_out <- classify_model(container, maxent_model)

```

****

<br>

# 6. Evaluate the results.

### A. Create a new dataframe with the true and expected classifications.

```{r}

labels.out <- data.frame( 
  correct.label = labels.corpus[round(P*N+1):N], 
  svm = as.character(svm_out[,1]), 
  #tree = as.character(tree_out[,1]), 
  maxent = as.character(maxent_out[,1]), 
  stringsAsFactors = F)

head(labels.out)

```

<br>

### B. Performance of Support Vector Machine Model

```{r}

table(labels.out[,1] == labels.out[,2]) 

prop.table(table(labels.out[,1] == labels.out[,2])) 

```

<br>

### C. Performance of Maximum Entropy Model

```{r}

table(labels.out[,1] == labels.out[,3]) 

prop.table(table(labels.out[,1] == labels.out[,3])) 

```

<br>

****

# 7. Evaluation



