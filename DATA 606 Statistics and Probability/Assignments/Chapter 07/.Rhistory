combined.df$type <- NA
combined.df$type[1:1396] <- "spam"
combined.df$type[1397:2796] <- "ham"
head(combined.df)
combined.df$text <-  str_replace_all(combined.df$text, "^c\\(", "") %>%
str_replace_all("[[:punct:]]", " ")
combined.df$text <- enc2utf8(combined.df$text)
head(combined.df)
combined.corpus <- VCorpus(VectorSource(combined.df$text))
print(combined.corpus)
# The dtm function requires that letters with diacritics -- essentially, letters t
corpus.dtm <- DocumentTermMatrix(combined.corpus)
combined.corpus <- Corpus(VectorSource(combined.df$text))
print(combined.corpus)
# The dtm function requires that letters with diacritics -- essentially, letters t
corpus.dtm <- DocumentTermMatrix(combined.corpus)
# corpus.tdm <- TermDocumentMatrix(clean.corpus)
print(corpus.dtm)
# inspect(corpus.tdm)
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The number of documents
N <- length(combined.corpus)
# The percentage of the data to partition
P <- 0.8
container <- create_container(corpus.dtm,
labels = labels.corpus,
trainSize = 1:(P*N),
testSize = (P*N+1):N,
virgin = F)
slotNames(container)
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The number of documents
N <- length(combined.corpus)
# The percentage of the data to partition
P <- 0.8
container <- create_container(corpus.dtm,
labels = labels.corpus,
trainSize = 1:round(P*N),
testSize = round(P*N+1):N,
virgin = F)
slotNames(container)
N
# Classify using the Support Vector Machines model
svm_model <- train_model(container, "SVM")
svm_out <- classify_model(container, svm_model)
# Classify using the Random Forest model
#tree_model <- train_model(container, "TREE")
#tree_out <- classify_model(container, tree_model)
# Classify using the Maximum Entropy model
#maxent_model <- train_model(container, "MAXENT")
#maxent_out <- classify_model(container, maxent_model)
# Classify using the Support Vector Machines model
svm_model <- train_model(container, "SVM")
svm_out <- classify_model(container, svm_model)
svm_out
# Classify using the Random Forest model
#tree_model <- train_model(container, "TREE")
#tree_out <- classify_model(container, tree_model)
# Classify using the Maximum Entropy model
#maxent_model <- train_model(container, "MAXENT")
#maxent_out <- classify_model(container, maxent_model)
combined.df$type <- NA
combined.df$type[1:1396] <- "spam"
combined.df$type[1397:2796] <- "ham"
combined.df$type <- factor(combined.df$type, levels = 2, labels = c("spam", "ham"))
combined.df$type <- NA
combined.df$type[1:1396] <- "spam"
combined.df$type[1397:2796] <- "ham"
combined.df$type <- factor(combined.df$type)
# Classify using the Support Vector Machines model
svm_model <- train_model(container, "SVM")
svm_out <- classify_model(container, svm_model)
svm_out
# Classify using the Random Forest model
#tree_model <- train_model(container, "TREE")
#tree_out <- classify_model(container, tree_model)
# Classify using the Maximum Entropy model
#maxent_model <- train_model(container, "MAXENT")
#maxent_out <- classify_model(container, maxent_model)
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The number of documents
N <- length(combined.corpus)
# The percentage of the data to partition
P <- 0.8
container <- create_container(corpus.dtm,
labels = labels.corpus,
trainSize = 1:round(P*N),
testSize = round(P*N+1):N,
virgin = F)
slotNames(container)
# Classify using the Support Vector Machines model
svm_model <- train_model(container, "SVM")
svm_out <- classify_model(container, svm_model)
svm_out
# Classify using the Random Forest model
#tree_model <- train_model(container, "TREE")
#tree_out <- classify_model(container, tree_model)
# Classify using the Maximum Entropy model
#maxent_model <- train_model(container, "MAXENT")
#maxent_out <- classify_model(container, maxent_model)
l <- length(combined.corpus)
trainSet <- l - round(P*N)
testSet <- l -(P*N+1)
l <- length(combined.corpus)
trainSet <- l - round(P*N)
testSet <- l -(P*N+1)
trainSet
testSet
trainSet <- N - round(P*N)
testSet <- N - (P*N+1)
trainSet
testSet
trainSet <- round(P*N)
testSet <- N - (P*N+1)
trainSet
testSet
trainSet <- round(P*N)
testSet <- N - round(P*N+1)
trainSet
testSet
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The number of documents
N <- length(combined.corpus)
# The percentage of the data to partition
P <- 0.8
trainSet <- round(P*N)
testSet <- N - round(P*N+1)
container <- create_container(corpus.dtm,
labels = labels.corpus,
trainSize = 1:round(P*N),
testSize = round(P*N+1):N,
virgin = F)
slotNames(container)
trainSet
testSet
combined.corpus <- Corpus(VectorSource(combined.df$text))
combined.corpus <- sample(combined.corpus)
print(combined.corpus)
labels.out <- data.frame(
correct.label = labels.corpus[1:round(P*N)],
svm = as.character(svm_out[,1]),
#tree = as.character(tree_out[,1]),
#maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
labels.out <- data.frame(
correct.label = labels.corpus[round(P*N+1):N],
svm = as.character(svm_out[,1]),
#tree = as.character(tree_out[,1]),
#maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
head(labels.out)
labels.out <- data.frame(
correct.label = labels.corpus[round(P*N+1):N],
svm = as.character(svm_out[,1]),
#tree = as.character(tree_out[,1]),
#maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
View(labels.out)
# Classify using the Support Vector Machines model
svm_model <- train_model(container, "SVM")
svm_out <- classify_model(container, svm_model)
# Classify using the Random Forest model
tree_model <- train_model(container, "TREE")
# Classify using the Support Vector Machines model
svm_model <- train_model(container, "SVM")
svm_out <- classify_model(container, svm_model)
# Classify using the Random Forest model
#tree_model <- train_model(container, "TREE")
#tree_out <- classify_model(container, tree_model)
# Classify using the Maximum Entropy model
maxent_model <- train_model(container, "MAXENT")
maxent_out <- classify_model(container, maxent_model)
labels.out <- data.frame(
correct.label = labels.corpus[round(P*N+1):N],
svm = as.character(svm_out[,1]),
#tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
View(labels.out)
labels.out <- data.frame(
correct.label = labels.corpus[round(P*N+1):N],
svm = as.character(svm_out[,1]),
#tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
head(labels.out)
table(labels.out[,1] == labels.out[,2])
table(labels.out[,1] == labels.out[,2])
prop.table(table(labels.out[,1] == labels.out[,2]))
table(labels.out[,1] == labels.out[,3])
prop.table(table(labels.out[,1] == labels.out[,3]))
labels.out <- data.frame(
correct.label = labels.corpus[round(P*N+1):N],
svm = as.character(svm_out[,1]),
#tree = as.character(tree_out[,1]),
maxent = as.character(maxent_out[,1]),
stringsAsFactors = F)
head(labels.out)
corpus.dtm <- DocumentTermMatrix(combined.corpus)
corpus.dtm <- removeSparseTerms(corpus.dtm, (1 - 10 / length(combined.corpus)))
print(corpus.dtm)
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The number of documents
N <- length(combined.corpus)
# The percentage of the data to partition
P <- 0.7
# Size of the training set
trainSize <- round(P*N)
# Size of the test set
testSize <- N - round(P*N+1)
# Create a container
container <- create_container(corpus.dtm,
labels = labels.corpus,
trainSize = 1:round(P*N),
testSize = round(P*N+1):N,
virgin = F)
slotNames(container)
trainSize
testSize
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The number of documents
N <- length(combined.corpus)
# The percentage of the data to partition
P <- 0.7
# Size of the training set
trainSize <- round(P*N)
trainSize
# Size of the test set
testSize <- N - round(P*N+1)
testSize
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The number of documents
N <- length(combined.corpus)
# The percentage of the data to partition
P <- 0.7
# Number of documents in the training set
trainSize <- round(P*N)
trainSize
# Number of documents in the test (holdout) set
testSize <- N - round(P*N+1)
testSize
# Specify the location of the "spam" and "ham" labels
labels.corpus <- combined.df$type
# The total number of documents
N <- length(combined.corpus)
# The percentage of the data to partition
P <- 0.7
# Number of documents in the training set
trainSize <- round(P*N)
trainSize
# Number of documents in the test (holdout) set
testSize <- N - round(P*N+1)
testSize
N
combined.df$text <-  str_replace_all(combined.df$text, "^c\\(", "") %>%
str_replace_all("[[:punct:]]", " ")
combined.df$text <- enc2utf8(combined.df$text)
head(combined.df, 1)
knitr::opts_chunk$set(echo = TRUE)
install.packages('data606')
knitr::opts_chunk$set(echo = TRUE)
library(DATA606)
knitr::opts_chunk$set(echo = TRUE)
library(DATA606)
startLab(Lab07)
knitr::opts_chunk$set(echo = TRUE)
library(DATA606)
startLab(Lab7)
knitr::opts_chunk$set(echo = TRUE)
library(DATA606)
startLab(lab7)
knitr::opts_chunk$set(echo = TRUE)
library(DATA606)
startLab(lab07)
knitr::opts_chunk$set(echo = TRUE)
library(DATA606)
startLab("Lab07")
knitr::opts_chunk$set(echo = TRUE)
library(DATA606)
startLab("Lab07")
knitr::opts_chunk$set(echo = TRUE)
library("DATA606")
startLab("Lab07")
knitr::opts_chunk$set(echo = TRUE)
library('DATA606')
startLab("Lab07")
getLabs()
startLab("Lab7")
u <- "C:/Users/Kavya/Desktop/Kavya/msds/DATA 606 Statistics and Probability/Assignments/Chapter 07"
setwd(u)
startLab("Lab7")
load("more/mlb11.RData")
cor(mlb11$runs, mlb11$at_bats)
plot_ss(x = mlb11$at_bats, y = mlb11$runs)
plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE)
m1 <- lm(runs ~ at_bats, data = mlb11)
summary(m1)
plot(mlb11$runs ~ mlb11$at_bats)
abline(m1)
plot(m1$residuals ~ mlb11$at_bats)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
hist(m1$residuals)
qqnorm(m1$residuals)
qqline(m1$residuals)  # adds diagonal line to the normal prob plot
load("more/mlb11.RData")
library(ggplot2)
View(head(mlb11))
qplot(mlb11$runs, mlb11$at_bats)
plot_ss(x = mlb11$at_bats, y = mlb11$runs)
plot_ss(mlb11$runs, mlb11$at_bats, showSquares = TRUE)
plot_ss(mlb11$runs, mlb11$at_bats, showSquares = TRUE)
plot_ss(mlb11$runs, mlb11$at_bats, showSquares = TRUE)
lm(formula = y ~ x, data = pts)
plot_ss(mlb11$runs, mlb11$at_bats, showSquares = TRUE)
m1 <- lm(runs ~ at_bats, data = mlb11)
summary(m1)
m2 <- lm(runs ~ homeruns, data = mlb11)
summary(m2)
m2 <- lm(homeruns ~ runs, data = mlb11)
summary(m2)
m2 <- lm(homeruns ~ runs, data = mlb11)
summary(m2)
View(mlb11)
−2789.2429+(0.6305*5579)
−2789.2429+(0.6305*5578)
lm(mlb11$wins, mlb11$runs)
lm(mlb11$wins, mlb11$runs, data=mlb11)
lm(mlb11$wins, mlb11$runs, data="mlb11")
qplot(mlb11$wins, mlb11$runs)
qplot(mlb11$hits, mlb11$runs)
qplot(mlb11$bat_avg, mlb11$runs)
qplot(mlb11$strikeouts, mlb11$runs)
qplot(mlb11$stolen_bases, mlb11$runs)
qplot(mlb11$new_onbase, mlb11$runs)
qplot(mlb11$new_slug, mlb11$runs)
qplot(mlb11$wins, mlb11$runs)
qplot(mlb11$stolen_bases, mlb11$runs)
qplot(mlb11$bat_avg, mlb11$runs)
qplot(mlb11$bat_avg, mlb11$runs)
m3 <- lm(runa ~ bat_avg, data = mlb11)
m3 <- lm(runs ~ bat_avg, data = mlb11)
summary(m3)
qplot(mlb11$bat_avg, mlb11$runs)
abline(m3)
plot(mlb11$bat_avg ~ mlb11$runs)
abline(m3)
m3 <- lm(bat_avg ~ runs, data = mlb11)
summary(m3)
plot(mlb11$bat_avg ~ mlb11$runs)
abline(m3)
plot(mlb11$bat_avg*100 ~ mlb11$runs)
abline(m3)
m3 <- lm(bat_avg*100 ~ runs, data = mlb11)
summary(m3)
plot(mlb11$bat_avg*100 ~ mlb11$runs)
abline(m3)
h1 <- lm(hits ~ runs, data = mlb11)
summary(h1)
h2 <- lm(homeruns ~ runs, data = mlb11)
summary(h2)
h3 <- lm(strikeouts ~ runs, data = mlb11)
summary(h3)
h4 <- lm(stolen_bases ~ runs, data = mlb11)
summary(h4)
h5 <- lm(wins ~ runs, data = mlb11)
summary(h5)
h1 <- lm(hits ~ runs, data = mlb11)
summary(h1)
plot(mlb11$hits ~ mlb11$runs)
abline(h1)
h6 <- lm(new_onbase ~ runs, data = mlb11)
summary(h6)
plot(mlb11$new_onbase ~ mlb11$runs)
abline(h6)
h7 <- lm(new_slug ~ runs, data = mlb11)
summary(h7)
plot(mlb11$new_slug ~ mlb11$runs)
abline(h7)
h8 <- lm(new_obs ~ runs, data = mlb11)
summary(h8)
plot(mlb11$new_obs ~ mlb11$runs)
abline(h8)
plot(mlb11$bat_avg*100 ~ mlb11$runs)
abline(m3)
plot(mlb11$bat_avg*100 ~ mlb11$runs)
plot(h8$residuals ~ mlb11$new_obs)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
plot(h8$residuals ~ mlb11$new_obs)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
hist(h8$residuals)
plot(h8$residuals ~ mlb11$new_obs)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
hist(h8$residuals)
qqnorm(h8$residuals)
qqline(h8$residuals)  # adds diagonal line to the normal prob plot
knitr::opts_chunk$set(echo = TRUE)
library('DATA606')
rm(list = ls())
0.67 * (171.14 / 107.2)
x_26 <- 107.2
y_26 <- 171.14
#Slope
b1_26 <- 0.67 * (x_26 / y_26)
y_26 <- b0_26 + b1_26 * x_26
x_26 <- 107.2
y_26 <- 171.14
#Slope
b1_26 <- 0.67 * (x_26 / y_26)
-b0_26 <- b0_26 + b1_26 * x_26 + y_26
x_26 <- 107.2
y_26 <- 171.14
#Slope
b1_26 <- 0.67 * (x_26 / y_26)
b0_26 <- b0_26 + b1_26 * x_26 + y_26
x_26 <- 107.2
y_26 <- 171.14
#Slope
b1_26 <- 0.67 * (x_26 / y_26)
b0_26 <- b1_26 * x_26 + y_26
b0_26*-1
x_26 <- 107.2
y_26 <- 171.14
#Slope
b1_26 <- 0.67 * (x_26 / y_26)
b0_26 <- (b1_26 * x_26 - y_26)*-1
b0_26
x_26 <- 107.2
y_26 <- 171.14
#Slope
b1_26 <- 0.67 * (x_26 / y_26)
b1_26
b0_26 <- (b1_26 * x_26 - y_26)*-1
b0_26
# Coordinates from mean
x.26 <- 107.2
y.26 <- 171.14
# Correlation coefficient
R.26 <- 0.67
# Slope
b1.26 <- R.26 * (x.26 / y.26)
b1.26
# Intercept
b0.26 <- (b1.26 * x.26 - y.26)*-1
b0.26
yhat.26 <- b1.26 * 100 + 126.1503
yhat.26
168 - 160
# Coordinates from mean
x.26 <- 107.2
y.26 <- 171.14
# Correlation coefficient
R.26 <- 0.67
# Slope
b1.26 <- R.26 * (x.26 / y.26)
b1.26
# Intercept
b0.26 <- (b1.26 * x.26 - y.26)*-1
b0.26
b1.26*100 + b0.26
# Coordinates from mean
x.26 <- 107.2
y.26 <- 171.14
# Correlation coefficient
R.26 <- 0.67
# Slope
b1.26 <- R.26 * (x.26 / y.26)
b1.26
# Intercept
b0.26 <- (b1.26 * x.26 - y.26)*-1
b0.26
#
b1.26*10 + b0.26
# Coordinates from mean
x.26 <- 107.2
y.26 <- 171.14
# Correlation coefficient
R.26 <- 0.67
# Slope
b1.26 <- R.26 * (x.26 / y.26)
b1.26
# Intercept
b0.26 <- (b1.26 * x.26 - y.26)*-1
b0.26
#
b1.26*1 + b0.26
# Coordinates from mean
x.26 <- 107.2
y.26 <- 171.14
# Correlation coefficient
R.26 <- 0.67
# Slope
b1.26 <- R.26 * (x.26 / y.26)
b1.26
# Intercept
b0.26 <- (b1.26 * x.26 - y.26)*-1
b0.26
#
b1.26*10 + b0.26
# Given values
x.40 <- -0.0883
y.40 <- 3.9983
b0.40 <- 4.010
# Slope
b1.40 <- (y.40 - b0.40) / x.40
b1.40
